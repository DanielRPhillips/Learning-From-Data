

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>2.1. Lecture 4: Clean up and Parameter estimation &#8212; Learning from data</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"N": ["\\mathbb{N}"], "Z": ["\\mathbb{Z}"], "floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"], "alphavec": ["\\boldsymbol{\\alpha}"], "muvec": ["\\boldsymbol{\\mu}"], "phivec": ["\\boldsymbol{\\phi}"], "sigmavec": ["\\boldsymbol{\\sigma}"], "Sigmavec": ["\\boldsymbol{\\Sigma}"], "thetavec": ["\\boldsymbol{\\theta}"], "thetavechat": ["\\widehat\\thetavec"], "avec": ["\\boldsymbol{a}"], "Bvec": ["\\boldsymbol{B}"], "fvec": ["\\boldsymbol{f}"], "mvec": ["\\boldsymbol{m}"], "qvec": ["\\boldsymbol{q}"], "rvec": ["\\boldsymbol{r}"], "uvec": ["\\boldsymbol{u}"], "wvec": ["\\boldsymbol{w}"], "xvec": ["\\boldsymbol{x}"], "yvec": ["\\boldsymbol{y}"], "Lra": ["\\Longrightarrow"], "abar": ["\\overline a"], "Xbar": ["\\overline X"], "alphahat": ["\\widehat\\alpha"], "Hhat": ["\\hat H"], "yth": ["y_{\\text{th}}"], "yexp": ["y_{\\text{exp}}"], "ym": ["y_{\\text{m}}"], "gs": ["{0}"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'content/Parameter_estimation/lecture_04';</script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="2.2. Visualization of the Central Limit Theorem" href="../../notebooks/Basics/visualization_of_CLT.html" />
    <link rel="prev" title="2. Bayesian parameter estimation" href="param_est.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../about.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/buqeye_logo_web.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/buqeye_logo_web.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../about.html">
                    About this Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Course overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Course/overview.html">Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Topics</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../Basics/basics.html">1. Basics of Bayesian statistics</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_01.html">1.1. Lecture 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule.html">1.2. Checking the sum and product rules, and their consequences</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/Exploring_pdfs.html">1.3. Exploring PDFs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_02.html">1.4. Lecture 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/Bayesian_updating_coinflip_interactive.html">1.5. Interactive Bayesian updating: coin flipping example</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Basics/lecture_03.html">1.6. Lecture 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/parameter_estimation_Gaussian_noise.html">1.7. Parameter estimation example: Gaussian noise and averages I</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/radioactive_lighthouse_exercise.html">1.8. Radioactive lighthouse problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes.html">1.9. Standard medical example by applying Bayesian rules of probability</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="param_est.html">2. Bayesian parameter estimation</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2.1. Lecture 4: Clean up and Parameter estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/visualization_of_CLT.html">2.2. Visualization of the Central Limit Theorem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise-2.html">2.3. Parameter estimation example: Gaussian noise and averages II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/Assignment_extending_radioactive_lighthouse.html">2.4. Assignment: 2D radioactive lighthouse location using MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_05.html">2.5. Lecture 5</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_fitting_straight_line_I.html">2.6. Parameter estimation example: fitting a straight line</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/demo-ModelValidation.html">2.7. Linear Regression and Model Validation demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="lecture_06.html">2.8. Lecture 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/amplitude_in_presence_of_background.html">2.9. Amplitude of a signal in the presence of background</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/Assignment_parameter_estimation_followups.html">2.10. Assignment: Follow-ups to Parameter Estimation notebooks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/exercise_LinearRegression.html">2.11. Linear Regression exercise</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/linear_algebra_games_I.html">2.12. Linear algebra games including SVD for PCA</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/assignments/fluctuation_trend_with_number_of_points_and_data_errors.html">2.13. Follow-up: fluctuation trends with # of points and data errors</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MCMC_sampling_I/MCMC_sampling_I.html">3. MCMC sampling I</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_I/lecture_07.html">3.1. Lecture 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/Metropolis_Poisson_example.html">3.2. Metropolis-Hasting MCMC sampling of a Poisson distribution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_I/lecture_08.html">3.3. Lecture 8</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-random-walk-and-sampling.html">3.4. Exercise: Random walk</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Why_Bayes_is_better/bayes_is_better.html">4. Why Bayes is better</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_09.html">4.1. Lecture 9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/bayes_billiard.html">4.2. A Bayesian Billiard game</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_10.html">4.3. Lecture 10</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/parameter_estimation_fitting_straight_line_II.html">4.4. Parameter estimation example: fitting a straight line II</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_11.html">4.5. Lecture 11</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/error_propagation_to_functions_of_uncertain_parameters.html">4.6. Error propagation: Example 3.6.2 in Sivia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Basics/correlation_intuition.html">4.7. Building intuition about correlations (and a bit of Python linear algebra)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_12.html">4.8. Lecture 12</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_I/MCMC-diagnostics.html">4.9. Overview: MCMC Diagnostics</a></li>

<li class="toctree-l2"><a class="reference internal" href="../Why_Bayes_is_better/lecture_13.html">4.11. Lecture 13</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Why_Bayes_is_better/dealing_with_outliers.html">4.12. Dealing with outliers</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Model_selection/model_selection.html">5. Model selection</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_14.html">5.1. Lecture 14</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_15.html">5.2. Lecture 15</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Model_selection/Evidence_for_model_EFT_coefficients.html">5.3. Evidence calculation for EFT expansions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Model_selection/lecture_16.html">5.4. Lecture 16</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee.html">5.5. Example: Parallel tempering for multimodal distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/MCMC-parallel-tempering_ptemcee_vs_zeus.html">5.6. Example: Parallel tempering for multimodal distributions vs. zeus</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../MCMC_sampling_II/MCMC_sampling_II.html">6. MCMC sampling II</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_II/lecture_17.html">6.1. Lecture 17</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/chi_squared_tests.html">6.2. Quick check of the distribution of normal variables squared</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/Liouville_theorem_visualization.html">6.3. Liouville Theorem Visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/Orbital_eqs_with_different_algorithms.html">6.4. Solving orbital equations with different algorithms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../MCMC_sampling_II/lecture_18.html">6.5. Lecture 18</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_intro_updated.html">6.6. PyMC3 Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/MCMC_sampling_II/PyMC3_docs_getting_started_updated.html">6.7. Getting started with PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_Gaussian_noise_compare_samplers.html">6.8. Comparing samplers for a simple problem</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/mini-projects/zeus_multimodal.html">6.9. zeus: Sampling from multimodal distributions</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Gaussian_processes/gaussian_processes.html">7. Gaussian processes</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Gaussian_processes/lecture_19.html">7.1. Lecture 19</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/demo-GaussianProcesses.html">7.2. Gaussian processes demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/GaussianProcesses.html">7.3. Learning from data: Gaussian processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Gaussian_processes/Gaussian_processes_exercises.html">7.4. Exercise: Gaussian Process models with GPy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Gaussian_processes/lecture_20.html">7.5. Lecture 20</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Maximum_entropy/max_ent.html">8. Assigning probabilities</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Maximum_entropy/lecture_21.html">8.1. Lecture 21</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt.html">8.2. Ignorance pdfs: Indifference and translation groups</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/Pdfs_from_MaxEnt.html">8.3. MaxEnt for deriving some probability distributions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/MaxEnt_Function_Reconstruction.html">8.4. Maximum Entropy for reconstructing a function from its moments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Maximum_entropy/demo-MaxEnt.html">8.5. Making figures for Ignorance PDF notebook</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Machine_learning/machine_learning.html">9. Machine learning: Bayesian methods</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_22.html">9.1. Lecture 22</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_optimization.html">9.2. Bayesian Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_23.html">9.3. Lecture 23</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Neural_networks_explained.html">9.4. What Are Neural Networks?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_NeuralNet.html">9.5. Neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Forssen_tif285_demo-NeuralNet.html">9.6. Neural network classifier demonstration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Bayesian_neural_networks_tif285.html">9.7. Bayesian neural networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Machine_learning/lecture_24.html">9.8. Lecture 24</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/demo-Bayesian_neural_networks_tif285.html">9.9. Variational Inference: Bayesian Neural Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Machine_learning/Convolutional_neural_network_explained.html">9.10. What is a convolutional neural network?</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../SVD/svd.html">10. PCA, SVD, and all that</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../SVD/lecture_25.html">10.1. Lecture 25</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/SVD/linear_algebra_games_including_SVD.html">10.2. Linear algebra games including SVD for PCA</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mini-projects</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_I_toy_model_of_EFT.html">Mini-project I: Parameter estimation for a toy model of an EFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIa.html">Mini-project IIa: Model selection basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/model-selection_mini-project-IIb_How_many_lines_ptemcee.html">Mini-project IIb: How many lines are there?</a></li>

<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIa_bayesian_optimization.html">Mini-project IIIa: Bayesian optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/mini-projects/mini-project_IIIb_Bayesian_neural_networks_from_demo.html">Mini-project IIIb: Bayesian Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference material</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../zbibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../related_topics.html">Related topics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/installing_anaconda.html">Using Anaconda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Reference/using_github.html">Using GitHub</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Reference/python_jupyter.html">Python and Jupyter notebooks</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_01.html">Python and Jupyter notebooks: part 01</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Reference/Jupyter_Python_intro_02.html">Python and Jupyter notebooks: part 02</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../jb_tests.html">Examples: Jupyter jb-book</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notebook keys</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Basics/simple_sum_product_rule_KEY.html">Checking the sum and product rules, and their consequences <span style="color: red">Key</span></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Basics/medical_example_by_Bayes_KEY.html">Standard medical example by applying Bayesian rules of probability <span style="color: red">Key</span></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DanielRPhillips/LearningFromData" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DanielRPhillips/LearningFromData/issues/new?title=Issue%20on%20page%20%2Fcontent/Parameter_estimation/lecture_04.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/content/Parameter_estimation/lecture_04.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 4: Clean up and Parameter estimation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-near-ubiquity-of-gaussians">The near ubiquity of Gaussians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">The Gaussian is to statistics what the harmonic oscillator is to mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences">Consequences:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof">Proof:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook">Notebook:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-when-all-you-can-do-is-falsify">p-values: when all you can do is falsify</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-degree-of-belief-intervals-and-frequentist-confidence-intervals">Bayesian degree of belief intervals and frequentist confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-overview-comments">Parameter estimation overview comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-go-wrong-in-a-fit">What can go wrong in a fit?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook-fitting-a-line">Notebook: Fitting a line</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="lecture-4-clean-up-and-parameter-estimation">
<h1><span class="section-number">2.1. </span>Lecture 4: Clean up and Parameter estimation<a class="headerlink" href="#lecture-4-clean-up-and-parameter-estimation" title="Permalink to this heading">#</a></h1>
<section id="the-near-ubiquity-of-gaussians">
<h2>The near ubiquity of Gaussians<a class="headerlink" href="#the-near-ubiquity-of-gaussians" title="Permalink to this heading">#</a></h2>
<p>In the last lecture we considered a Gaussian distribution and
estimated its mean and variance. We are going to see a lot of Gaussian
distributions in this course. And indeed some people implicitly always
assume Gaussian distributions. So this seems like as good a place as
any to pause and consider why Gaussian distributions are a common
choice to describe noise, as well as thinking about the circumstances
in which that choice might be a poor one.</p>
<p>It turns out there are several reasons why one might choose a Gaussian
to describe a probability distribution. Here are two:</p>
<section id="the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">
<h3>The Gaussian is to statistics what the harmonic oscillator is to mechanics<a class="headerlink" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics" title="Permalink to this heading">#</a></h3>
<p>Suppose we have a probability distribution <span class="math notranslate nohighlight">\(p(x | D,I)\)</span> that is
unimodal (has only one hump), then one way to form a ``best estimate’’
for the variable <span class="math notranslate nohighlight">\(x\)</span> is to compute the maximum of the
distribution. (To save writing we denote the pdf of interest as <span class="math notranslate nohighlight">\(p(x)\)</span>
for a while hereafter.)</p>
<a class="bg-primary reference internal image-reference" href="../../_images/point_estimate_cartoon.png"><img alt="point estimate" class="bg-primary align-right" src="../../_images/point_estimate_cartoon.png" style="width: 250px;" /></a>
<p>We find this point, which we’ll denote by <span class="math notranslate nohighlight">\(x_0\)</span>, using calculus:</p>
<div class="math notranslate nohighlight">
\[
  \left.\frac{dp}{dx}\right|_{x_0} = 0
  \quad \mbox{with} \quad
    \left.\frac{d^2p}{dx^2}\right|_{x_0} &lt; 0 \ \text{(maximum)}.
\]</div>
<p>To characterize the posterior <span class="math notranslate nohighlight">\(p(x)\)</span>, we look nearby. We want to know
how sharp this maximum is: is <span class="math notranslate nohighlight">\(p(x)\)</span> sharply peaked around <span class="math notranslate nohighlight">\(x=x_0\)</span> or
is the maximum kind-of shallow? To work this out we’ll do a Taylor
expansion around <span class="math notranslate nohighlight">\(x=x_0\)</span>.
<span class="math notranslate nohighlight">\(p(x)\)</span> itself
varies too fast, but since <span class="math notranslate nohighlight">\(p(x)\)</span> is positive definite we can
Taylor expand <span class="math notranslate nohighlight">\(\log p\)</span> instead. (See the box below for a strict mathematical
reason why it’s a bad idea to directly Taylor expand <span class="math notranslate nohighlight">\(p(x)\)</span> around its
maximum.)</p>
<div class="math notranslate nohighlight">
\[
 \Longrightarrow\ L(x) \equiv \log p(x|D,I) = 
   L(x_0) + \left.\frac{dL}{dx}\right|_{x_0 = 0}
   + \frac{1}{2} \left.\frac{d^2L}{dx^2}\right|_{x_0 = 0}(x-x_0)^2 + \cdots
\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\left.\frac{d^2L}{dx^2}\right|_{x_0 = 0} &lt; 0\)</span>.
If we can neglect higher-order terms, then</p>
<div class="math notranslate nohighlight">
\[
  p(x| D,I) \approx A\, e^{\frac{1}{2}\left.\frac{d^2L}{dx^2}\right|_{x_0 = 0}(x-x_0)^2} ,
\]</div>
<p>with <span class="math notranslate nohighlight">\(A\)</span> a normalization factor. So in this general circumstance we get a Gaussian. Comparing to</p>
<div class="math notranslate nohighlight">
\[
  p(x|D,I) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/\sigma^2}
  \quad\Longrightarrow\quad
  \mu = x_0, \ \sigma = \left(-\left.\frac{d^2L}{dx^2}\right|_{x_0}\right)^{-1/2}
\]</div>
<ul class="simple">
<li><p>We usually quote <span class="math notranslate nohighlight">\(x = x_0 \pm \sigma\)</span>, because <em>if</em> it is a Gaussian this is <em>sufficient</em> to tell us the entire distribution and <span class="math notranslate nohighlight">\(n\)</span> standard deviations is <span class="math notranslate nohighlight">\(n\times \sigma\)</span>.</p></li>
<li><p>But for a Bayesian, the full posterior <span class="math notranslate nohighlight">\(p(x|D,I)\)</span> for all <span class="math notranslate nohighlight">\(x\)</span> is
the general result, and <span class="math notranslate nohighlight">\(x = x_0 \pm \sigma\)</span> may be only an
approximate characterization.</p></li>
</ul>
<div class="admonition-to-think-about admonition">
<p class="admonition-title">To think about …</p>
<p>What if <span class="math notranslate nohighlight">\(p(x|D,I)\)</span> is asymmetric? What if it is multimodal?</p>
</div>
<div class="admonition-p-or-log-p admonition">
<p class="admonition-title"><span class="math notranslate nohighlight">\(p\)</span> or <span class="math notranslate nohighlight">\(\log p\)</span>?</p>
<ul class="simple">
<li><p>We motivated Gaussian approximations from a Taylor expansion to quadratic order of the <em>logarithm</em> of a pdf.
What would go wrong if we directly expanded the pdf? Well, if we do
that we get:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
  p(x) \approx p(x_0) + \frac{1}{2}-\left.\frac{d^2p}{dx^2}\right|_{x_0}(x-x_0)^2
 \ \overset{x\pm\rightarrow\infty}{\longrightarrow} -\infty,
\]</div>
<p>i.e., we get something that diverges as <span class="math notranslate nohighlight">\(x\)</span> tends to either plus or
minus infinity.</p>
<a class="bg-primary reference internal image-reference" href="../../_images/pdf_expansion_cartoon.png"><img alt="pdf expansion" class="bg-primary align-center" src="../../_images/pdf_expansion_cartoon.png" style="width: 400px;" /></a>
<ul class="simple">
<li><p>A pdf must be normalizable and positive definite, so this approximation violates these conditions!</p></li>
</ul>
</div>
</section>
<section id="the-central-limit-theorem">
<h3>The Central Limit Theorem<a class="headerlink" href="#the-central-limit-theorem" title="Permalink to this heading">#</a></h3>
<p>Another reason a Gaussian pdf emerges in many calculations is because
the Central Limit Theorem states that all (or almost all) probability
distributions will eventually produce Gaussians if you take enough
data (or, equivalently, draw enough samples) from them.</p>
<p><em>Central Limit Theorem</em>: The sum of <span class="math notranslate nohighlight">\(n\)</span> random values drawn from any
pdf of finite variance <span class="math notranslate nohighlight">\(\sigma^2\)</span> tends as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span> to
be Gaussian distributed about the expectation value of the sum, with
variance <span class="math notranslate nohighlight">\(n \sigma^2\)</span>.</p>
<section id="consequences">
<h4>Consequences:<a class="headerlink" href="#consequences" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The mean of a large number of values becomes normally distributed
<em>regardless</em> of the probability distiburtion the values are drawn
from.</p></li>
<li><p>The binomial, Poisson, Student’s t-, and … distribtuions all kind
of look liek Gaussian distributions in the limit of a large number
of degrees of freedom.</p></li>
</ul>
</section>
<section id="proof">
<h4>Proof:<a class="headerlink" href="#proof" title="Permalink to this heading">#</a></h4>
<p>See worksheet.</p>
</section>
<section id="notebook">
<h4>Notebook:<a class="headerlink" href="#notebook" title="Permalink to this heading">#</a></h4>
<p>Look at <a class="reference internal" href="../../notebooks/Basics/visualization_of_CLT.html"><span class="doc std std-doc">Visualization of the Central Limit Theorem</span></a>.</p>
<p>Things to think about:</p>
<ul class="simple">
<li><p>What does ``large’’ number of degrees of freedom acutally mean? Does
it matter where we look?</p></li>
<li><p>Can you identify a case where the CLT will fail?</p></li>
</ul>
</section>
</section>
</section>
<section id="p-values-when-all-you-can-do-is-falsify">
<h2>p-values: when all you can do is falsify<a class="headerlink" href="#p-values-when-all-you-can-do-is-falsify" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>A common way for a frequentist to discuss a theory/model, or put a
bound on a parameter value, is to quote a p-value.</p></li>
<li><p>This is set up using something called the null hypothesis. Somewhat
perversely you should pick the null hypothesis to be the opposite of
what you want to prove. So if you want to discover the Higgs boson,
the null hypothesis is that the Higgs boson does not exist.</p></li>
<li><p>Then you pick a level of proof you are comfortable with. For the
Higgs boson (and for many other particle physics experiments) it is
`` 5 sigma’’. How do you think we convert this statement to a
probability?</p></li>
<li><p>One minus the resulting probability is called the <span class="math notranslate nohighlight">\(p\)</span>-value. We will
denote it <span class="math notranslate nohighlight">\(p_{\rm crit}\)</span>. There is nothing
God-given about it. It is a standard (like ``beyond a resaonable
doubt”) that has been established in a research community for
determining that something is (likely) going on.</p></li>
<li><p>You then take data and compute <span class="math notranslate nohighlight">\(p(D|null hypothesis)\)</span>. If <span class="math notranslate nohighlight">\(p(D|null
hypothesis) &lt; p_{\rm crit}\)</span> then you conclude that the ``the null
hypothesis is rejected at the <span class="math notranslate nohighlight">\(1- p_{\rm crit}\)</span> level’’.</p></li>
<li><p>Note that if <span class="math notranslate nohighlight">\(p(D|null hypothesis) &gt; p_{\rm crit}\)</span> you cannot
conculde that the null hypothesis is true. It just means ``no effect
was observeed”.</p></li>
<li><p>Look at
<a class="reference internal" href="../../notebooks/Basics/Bayesian_updating_coinflip_interactive.html"><span class="doc std std-doc">Interactive Bayesian updating: coin flipping example</span></a>. Pick
a <span class="math notranslate nohighlight">\(p\)</span>-value. If  <span class="math notranslate nohighlight">\(H=0.4\)</span> work out how many coin tosses it would take
to reject the null hypothesis that it’s a fair coin (<span class="math notranslate nohighlight">\(H=0.5\)</span>) at
that <span class="math notranslate nohighlight">\(p\)</span>-value.</p></li>
</ul>
</section>
<section id="bayesian-degree-of-belief-intervals-and-frequentist-confidence-intervals">
<h2>Bayesian degree of belief intervals and frequentist confidence intervals<a class="headerlink" href="#bayesian-degree-of-belief-intervals-and-frequentist-confidence-intervals" title="Permalink to this heading">#</a></h2>
<p>In class on Wednesday we also talked about the difference between the
68% degree of belief interval for the most likely value (in that case
the bias weighting of the coin) and a frequentist <span class="math notranslate nohighlight">\(1 \sigma\)</span> confidence
interval.</p>
<ul class="simple">
<li><p>First point is that <span class="math notranslate nohighlight">\(1 \sigma=68\)</span>% assumes a Gaussian distribution
around the maximum of the posterior (cf. above). While this will
often work out okay, it may not. And, as we seek to translate,
<span class="math notranslate nohighlight">\(n \sigma\)</span> intervals into DoB statements, assuming a Gaussian
becomes more and more questionable the higher <span class="math notranslate nohighlight">\(n\)</span> is. (Why?)</p></li>
<li><p>But the second point is more philosophical (meta-statistical?). One
interval is a statement about <span class="math notranslate nohighlight">\(p(x|D,I)\)</span>, while the other is a
statement about <span class="math notranslate nohighlight">\(p(D|x,I)\)</span>.</p></li>
<li><p>(Note that because the conversion between the two pdfs requires the
use of Bayes’ theorem the Bayesian interval may be affected by the
choice of the prior.)</p></li>
<li><p>Bayesian version is easy; a 68% credible interval or Bayesian
confidence interval or degree-of-belief (DoB) interval is: given
some data and some information <span class="math notranslate nohighlight">\(I\)</span>, there is a 68% chance (probability) that the interval contains the true parameter.</p></li>
<li><p>Frequentist 68% confidence interval</p>
<ul>
<li><p>Assuming the model (contained in <span class="math notranslate nohighlight">\(I\)</span>) and the value of the
parameter <span class="math notranslate nohighlight">\(x\)</span> then if we do the experiment a large number of
times then 68% of them will produce data in that interval.</p></li>
<li><p>So the <em>parameter</em> is fixed (no pdf) and the confidence
interval is a statement about data</p></li>
<li><p>Frequentists will try to make statements about parameters, but
they end up a bit tangled, e.g., “There is a 68% probability
that when I compute a confidence interval from data of this sort
that the true value of <span class="math notranslate nohighlight">\(\theta\)</span> will fall within the
(hypothetical) space of observations.”</p></li>
</ul>
</li>
<li><p>For a one-dimensional posterior that is symmetric, it is clear how to define the <span class="math notranslate nohighlight">\(d\%\)</span> confidence interval.</p>
<ul>
<li><p>Algorithm: start from the center, step outward on both sides, stop when <span class="math notranslate nohighlight">\(d\%\)</span> is enclosed.</p></li>
<li><p>For a two-dimensional posterior, need a way to integrate from the top. (Could lower a plane, as desribed below for HPD.)</p></li>
</ul>
</li>
<li><p>What if asymmetic or multimodal? Two of the possible choices:</p>
<ul>
<li><p>Equal-tailed interval (central interval): the area above and below the interval are equal.</p></li>
<li><p>Highest posterior density (HPD) region: posterior density for
every point is higher than the posterior density for any point
outside the
interval. [E.g., lower a horizontal line over the distribution until the desired interval percentage is covered by regions above the line.]</p></li>
</ul>
</li>
</ul>
</section>
<section id="parameter-estimation-overview-comments">
<h2>Parameter estimation overview comments<a class="headerlink" href="#parameter-estimation-overview-comments" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>In general terms, “parameter estimation” in physics means obtaining values for parameters (i.e., constants) that appear in a theoretical model that describes data. (Exceptions exist, of course.)</p></li>
<li><p>Examples:</p>
<ul>
<li><p>couplings in a Hamiltonian</p></li>
<li><p>coefficients of a polynomial or exponential model of data</p></li>
<li><p>parameters describing a peak in a measured spectrum, such as the peak height and width (e.g., fitting a Lorentzian line shape) and the size of the background</p></li>
<li><p>cosmological parameters such as the Hubble constant</p></li>
</ul>
</li>
<li><p>Conventionally this process is known as “fitting the parameters” and the goal is to find the “best fit” and maybe error bars.</p></li>
<li><p>We will make particular interpretations of these phrases from our Bayesian point of view.</p></li>
<li><p>Plan: set up the problem and look at how familiar ideas like “least-squares fitting” show up from a Bayesian perspective.</p></li>
<li><p>As we proceed, we’ll make the case that for physics a Bayesian
approach is particular well suited.</p></li>
</ul>
</section>
<section id="what-can-go-wrong-in-a-fit">
<h2>What can go wrong in a fit?<a class="headerlink" href="#what-can-go-wrong-in-a-fit" title="Permalink to this heading">#</a></h2>
<p>As a teaser, let’s ask: what can go wrong in a fit?</p>
<a class="bg-primary reference internal image-reference" href="../../_images/over_under_fitting_cartoon.png"><img alt="bootstrapping" class="bg-primary align-center" src="../../_images/over_under_fitting_cartoon.png" style="width: 400px;" /></a>
<p>Bayesian methods can identify and prevent both underfitting (model is not complex enough to describe the fit data) or overfitting (model tunes to data fluctuations or terms are underdetermined, leading to them playing off each other).<br />
<span class="math notranslate nohighlight">\(\Longrightarrow\)</span> we’ll see how this plays out.</p>
</section>
<section id="notebook-fitting-a-line">
<h2>Notebook: Fitting a line<a class="headerlink" href="#notebook-fitting-a-line" title="Permalink to this heading">#</a></h2>
<p>Look at <a class="reference internal" href="../../notebooks/Parameter_estimation/parameter_estimation_fitting_straight_line_I.html"><span class="doc std std-doc">Parameter estimation example: fitting a straight line</span></a>.</p>
<p>Annotations of the notebook:</p>
<ul>
<li><p>same imports as before</p></li>
<li><p>assume we create data <span class="math notranslate nohighlight">\(y_{\rm exp}\)</span> (“exp” for “experiment”) from an underlying model of the form</p>
<div class="math notranslate nohighlight">
\[
      y_{\rm exp}(x) = m_{\rm true} x + b_{\rm true} + \mbox{Gaussian noise}
    \]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
     \boldsymbol{\theta}_{\rm true} = [b_{\rm true}, m_{\rm true}]
      = [\text{intercept, slope}]_{\rm true}
    \]</div>
</li>
<li><p>The Gaussian noise is taken to have mean <span class="math notranslate nohighlight">\(\mu=0\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma = dy\)</span> independent of <span class="math notranslate nohighlight">\(x\)</span>. This is implemented as
<code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">+=</span> <span class="pre">dy</span> <span class="pre">*</span> <span class="pre">rand.randn(N)</span></code> (note <code class="docutils literal notranslate"><span class="pre">randn</span></code>).</p></li>
<li><p>The <span class="math notranslate nohighlight">\(x_i\)</span> points themselves are also chosen randomly according to a uniform distribution <span class="math notranslate nohighlight">\(\Longrightarrow\)</span> <code class="docutils literal notranslate"><span class="pre">rand.rand(N)</span></code>.</p></li>
<li><p>Here we are using the <code class="docutils literal notranslate"><span class="pre">numpy</span></code> random number generators while we will mostly use those from <code class="docutils literal notranslate"><span class="pre">scipy.stats</span></code> elsewhere.</p></li>
</ul>
<p>The theoretical model <span class="math notranslate nohighlight">\(y_{\rm th}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[
   y_{\rm th} = m x + b, \quad \mbox{with}\ \theta = [b, m]
\]</div>
<p>So in the sense of distributions (i.e., not an algebraic equation),</p>
<div class="math notranslate nohighlight">
\[
  y_{\rm exp} = y_{\rm th} + \delta y_{\rm exp} + \delta y_{\rm th}
\]</div>
<ul>
<li><p>The last term, which is the model discrepancy (or “theory error”) will be critically important in many applications, but has often been neglected. More on this later!</p></li>
<li><p>Here we’ll take <span class="math notranslate nohighlight">\(\delta y_{\rm th}\)</span> to be negligible, which means that</p>
<div class="math notranslate nohighlight">
\[
      y_i \sim \mathcal{N}(y_{\rm th}(x_i;\boldsymbol{\theta}), dy^2)
    \]</div>
<ul class="simple">
<li><p>The notation here means that the random variable <span class="math notranslate nohighlight">\(y_i\)</span> is drawn from a normal (i.e., Gaussian) distribution with mean <span class="math notranslate nohighlight">\(y_{\rm th}(x_i;\boldsymbol{\theta})\)</span> (first entry) and variance <span class="math notranslate nohighlight">\(dy^2\)</span> (second entry).</p></li>
<li><p>For a long list of other probability distributions, see Appendix A of BDA3, which is what everyone calls Ref. <span id="id1">[<a class="reference internal" href="../zbibliography.html#id3" title="A. Gelman, J.B. Carlin, H.S. Stern, D.B. Dunson, A. Vehtari, and D.B. Rubin. Bayesian Data Analysis, Third Edition. Chapman &amp; Hall/CRC Texts in Statistical Science. Taylor &amp; Francis, 2013. URL: http://www.stat.columbia.edu/~gelman/book/BDA3.pdf.">GCS+13</a>]</span>.</p></li>
</ul>
</li>
<li><p>We are assuming independence here. Is that a reasonable assumption?</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "DanielRPhillips/LearningFromData",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./content/Parameter_estimation"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="param_est.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Bayesian parameter estimation</p>
      </div>
    </a>
    <a class="right-next"
       href="../../notebooks/Basics/visualization_of_CLT.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">2.2. </span>Visualization of the Central Limit Theorem</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-near-ubiquity-of-gaussians">The near ubiquity of Gaussians</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-gaussian-is-to-statistics-what-the-harmonic-oscillator-is-to-mechanics">The Gaussian is to statistics what the harmonic oscillator is to mechanics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-central-limit-theorem">The Central Limit Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#consequences">Consequences:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#proof">Proof:</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook">Notebook:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#p-values-when-all-you-can-do-is-falsify">p-values: when all you can do is falsify</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-degree-of-belief-intervals-and-frequentist-confidence-intervals">Bayesian degree of belief intervals and frequentist confidence intervals</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-estimation-overview-comments">Parameter estimation overview comments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-can-go-wrong-in-a-fit">What can go wrong in a fit?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notebook-fitting-a-line">Notebook: Fitting a line</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dick Furnstahl
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2021.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>